{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom skimage.feature import local_binary_pattern\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom skimage.feature import hog\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:53:54.883514Z","iopub.execute_input":"2023-06-14T19:53:54.883901Z","iopub.status.idle":"2023-06-14T19:53:54.890045Z","shell.execute_reply.started":"2023-06-14T19:53:54.883848Z","shell.execute_reply":"2023-06-14T19:53:54.889087Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"categories = ['jogging' , \n              'sitting' , \n              'standing' , \n              'walking']\ndata_directory = '/kaggle/input/data1/Dataset_images'\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:54:05.468329Z","iopub.execute_input":"2023-06-14T19:54:05.468690Z","iopub.status.idle":"2023-06-14T19:54:05.473272Z","shell.execute_reply.started":"2023-06-14T19:54:05.468659Z","shell.execute_reply":"2023-06-14T19:54:05.472223Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Step 2: Data Preprocessing\nimage_size = (256, 256)\ndataset = []\nlabels = []\n\nfor category in categories:\n    folder_path = os.path.join(data_directory, category)\n    for filename in os.listdir(folder_path):\n        image_path = os.path.join(folder_path, filename)\n        image = load_img(image_path, target_size=image_size)\n        image_array = img_to_array(image)\n        dataset.append(image_array)\n        labels.append(categories.index(category))\n\ndataset = np.array(dataset)\nlabels = np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:54:10.164411Z","iopub.execute_input":"2023-06-14T19:54:10.164768Z","iopub.status.idle":"2023-06-14T19:54:37.466336Z","shell.execute_reply.started":"2023-06-14T19:54:10.164739Z","shell.execute_reply":"2023-06-14T19:54:37.465320Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"hog_features = []\nhog_labels = []\n\nfor image, label in zip(dataset, labels):\n    \n    image = cv2.cvtColor(image , cv2.COLOR_RGB2GRAY)\n\n    # Apply HOG feature extraction\n    features = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False,)\n\n    # Append the features and label to the respective lists\n    hog_features.append(features)\n    hog_labels.append(label)\n\n# Convert the feature and label lists to numpy arrays\nhog_features = np.array(hog_features)\nhog_labels = np.array(hog_labels)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:36:30.608545Z","iopub.execute_input":"2023-06-14T19:36:30.609124Z","iopub.status.idle":"2023-06-14T19:38:13.947930Z","shell.execute_reply.started":"2023-06-14T19:36:30.609088Z","shell.execute_reply":"2023-06-14T19:38:13.946907Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(hog_features, hog_labels, test_size=0.2, random_state=42)\n\n\nk = 10  \nknn = KNeighborsClassifier(n_neighbors=k)\n\n## Scaling the data\nscaler = StandardScaler()\nX_train_1 = scaler.fit_transform(X_train_1)\nX_test_1 = scaler.transform(X_test_1)\nknn.fit(X_train_1, y_train_1)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:39:52.058436Z","iopub.execute_input":"2023-06-14T19:39:52.058836Z","iopub.status.idle":"2023-06-14T19:39:53.025351Z","shell.execute_reply.started":"2023-06-14T19:39:52.058804Z","shell.execute_reply":"2023-06-14T19:39:53.024202Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"KNeighborsClassifier(n_neighbors=10)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=10)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"folder_path = '/kaggle/input/test-images/test_images'\n\n\n# Get the list of image file names in the folder\nimage_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n\n# Initialize an empty list to store the resized images\ntest_images = []\n\n# Specify the desired image size\ndesired_size = (256, 256)  \n\n# Loop through the image files\nfor image_file in image_files:\n    # Read the image using OpenCV\n    image_path = os.path.join(folder_path, image_file)\n    image = cv2.imread(image_path)\n\n    # Resize the image to the desired size\n    resized_image = cv2.resize(image, desired_size)\n\n    # Append the resized image to the list\n    test_images.append(resized_image)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:54:56.112733Z","iopub.execute_input":"2023-06-14T19:54:56.113206Z","iopub.status.idle":"2023-06-14T19:54:58.700191Z","shell.execute_reply.started":"2023-06-14T19:54:56.113169Z","shell.execute_reply":"2023-06-14T19:54:58.699176Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Set up a timer\nstart_time = time.time()\n\n# Number of frames processed\nframe_count = 0\n\n# Inference loop\nfor frame in test_images:\n    \n    image = cv2.cvtColor(frame , cv2.COLOR_RGB2GRAY)\n    \n    feature  = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2),\n                          visualize=False)\n    feature = feature.reshape(1, -1)\n    \n    pred = knn.predict(feature)\n   \n    frame_count += 1\n\n# Calculate elapsed time\nelapsed_time = time.time() - start_time\n\n# Calculate FPS\nfps = frame_count / elapsed_time\n\n# Print the FPS\nprint(\"FPS: {:.2f}\".format(fps))","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:47:52.006451Z","iopub.execute_input":"2023-06-14T19:47:52.006843Z","iopub.status.idle":"2023-06-14T19:48:23.978503Z","shell.execute_reply.started":"2023-06-14T19:47:52.006811Z","shell.execute_reply":"2023-06-14T19:48:23.977532Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"FPS: 3.13\n","output_type":"stream"}]},{"cell_type":"code","source":"def extract_lbp_features(image):\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Compute LBP feature\n    radius = 1  # LBP neighborhood radius\n    n_points = 8 * radius  # Number of points to sample on the LBP circle\n    lbp = local_binary_pattern(image, n_points, radius, method='uniform')\n\n    # Calculate the histogram of LBP\n    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n\n    # Normalize the histogram\n    hist = hist.astype(\"float\")\n    hist /= (hist.sum() + 1e-7)\n\n    # Return the LBP histogram as feature vector\n    return hist","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:55:11.048903Z","iopub.execute_input":"2023-06-14T19:55:11.049299Z","iopub.status.idle":"2023-06-14T19:55:11.058922Z","shell.execute_reply.started":"2023-06-14T19:55:11.049265Z","shell.execute_reply":"2023-06-14T19:55:11.057667Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Initialize an empty list to store the LBP features\nlbp_features = []\n\n# Iterate over each image in the dataset and extract LBP features\nfor image in dataset:\n    lbp_features.append(extract_lbp_features(image))\n\n# Convert the features list to a numpy array\nlbp_features = np.array(lbp_features)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:55:13.180670Z","iopub.execute_input":"2023-06-14T19:55:13.181039Z","iopub.status.idle":"2023-06-14T19:56:23.885724Z","shell.execute_reply.started":"2023-06-14T19:55:13.181009Z","shell.execute_reply":"2023-06-14T19:56:23.884646Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/skimage/feature/texture.py:353: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"## KNN with standard scaling\n\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n\n\nk = 10  \nknn = KNeighborsClassifier(n_neighbors=k)\n\n## Scaling the data\nscaler = StandardScaler()\nX_train_1 = scaler.fit_transform(X_train_1)\nX_test_1 = scaler.transform(X_test_1)\nknn.fit(X_train_1, y_train_1)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:56:37.987548Z","iopub.execute_input":"2023-06-14T19:56:37.987919Z","iopub.status.idle":"2023-06-14T19:56:38.010608Z","shell.execute_reply.started":"2023-06-14T19:56:37.987884Z","shell.execute_reply":"2023-06-14T19:56:38.009707Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"KNeighborsClassifier(n_neighbors=10)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=10)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Set up a timer\nstart_time = time.time()\n\n# Number of frames processed\nframe_count = 0\n\n# Inference loop\nfor frame in test_images:\n    \n    \n    feature  = extract_lbp_features(frame)\n    feature = feature.reshape(1, -1)\n    \n    pred = knn.predict(feature)\n   \n    frame_count += 1\n\n# Calculate elapsed time\nelapsed_time = time.time() - start_time\n\n# Calculate FPS\nfps = frame_count / elapsed_time\n\n# Print the FPS\nprint(\"FPS: {:.2f}\".format(fps))","metadata":{"execution":{"iopub.status.busy":"2023-06-14T19:58:23.492442Z","iopub.execute_input":"2023-06-14T19:58:23.492799Z","iopub.status.idle":"2023-06-14T19:58:26.303609Z","shell.execute_reply.started":"2023-06-14T19:58:23.492769Z","shell.execute_reply":"2023-06-14T19:58:26.302531Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"FPS: 35.65\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}