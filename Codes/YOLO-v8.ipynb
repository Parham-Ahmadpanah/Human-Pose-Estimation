{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFgiwB22mFpK",
        "outputId": "a11dbdef-cf78-4560-f424-c984ab915744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ‚ö†Ô∏è 'yolo mode=checks' is deprecated. Use 'yolo checks' instead.\n",
            "\u001b[2K\r\u001b[2K\rUltralytics YOLOv8.0.117 üöÄ Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 23.3/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install roboflow\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "!yolo mode=checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rfy9oyTrmUBS"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image\n",
        "\n",
        "from roboflow import Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trCG_W-JmbRd",
        "outputId": "d1061403-7498-4c99-8c33-7568db5eb341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics<=8.0.20 is required but found version=8.0.117, to fix: `pip install ultralytics<=8.0.20`\n",
            "Downloading Dataset Version Zip in human-activity-detection-2-3 to yolov8: 100% [45715675 / 45715675] bytes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to human-activity-detection-2-3 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3974/3974 [00:00<00:00, 3975.22it/s]\n"
          ]
        }
      ],
      "source": [
        "rf = Roboflow(api_key=\"w9ZLwtgrHJyXSSswKk7K\")\n",
        "project = rf.workspace(\"ss-wq1cw\").project(\"human-activity-detection-2\")\n",
        "dataset = project.version(3).download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXHeaTWwmg5s",
        "outputId": "63a6acc1-b9fc-41b7-e6e1-40852d6a0fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: wandb: command not found\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to yolov8s.pt...\n",
            "100% 21.5M/21.5M [00:00<00:00, 161MB/s]\n",
            "Ultralytics YOLOv8.0.117 üöÄ Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/human-activity-detection-2-3/data.yaml, epochs=10, patience=50, batch=16, imgsz=256, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 17.6MB/s]\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "Model summary: 225 layers, 11137148 parameters, 11137132 gradients\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n",
            "100% 6.23M/6.23M [00:00<00:00, 81.6MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/human-activity-detection-2-3/train/labels... 1731 images, 0 backgrounds, 0 corrupt: 100% 1731/1731 [00:00<00:00, 2428.27it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/human-activity-detection-2-3/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/human-activity-detection-2-3/valid/labels... 164 images, 0 backgrounds, 0 corrupt: 100% 164/164 [00:00<00:00, 2004.11it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/human-activity-detection-2-3/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 256 train, 256 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/10      1.02G      1.233      1.486      1.246          6        256: 100% 109/109 [00:20<00:00,  5.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.11it/s]\n",
            "                   all        164        164       0.98      0.948       0.99      0.717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/10      1.08G      1.124     0.9427      1.203          6        256: 100% 109/109 [00:18<00:00,  5.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.86it/s]\n",
            "                   all        164        164      0.877      0.946      0.973      0.738\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/10      1.07G      1.111      0.885      1.207          4        256: 100% 109/109 [00:17<00:00,  6.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.42it/s]\n",
            "                   all        164        164      0.908      0.903      0.944      0.635\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/10      1.07G      1.074     0.8015      1.182          7        256: 100% 109/109 [00:19<00:00,  5.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  4.00it/s]\n",
            "                   all        164        164       0.98       0.95      0.992      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/10      1.07G      1.059     0.7544      1.169          8        256: 100% 109/109 [00:16<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.80it/s]\n",
            "                   all        164        164      0.967      0.971      0.993       0.76\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/10      1.07G      0.986     0.6706      1.137          8        256: 100% 109/109 [00:18<00:00,  5.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.79it/s]\n",
            "                   all        164        164      0.989      0.982      0.994      0.769\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/10      1.07G     0.9698     0.6325       1.13          9        256: 100% 109/109 [00:17<00:00,  6.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.95it/s]\n",
            "                   all        164        164      0.966      0.983       0.99      0.763\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/10      1.06G     0.9154      0.589      1.105          8        256: 100% 109/109 [00:18<00:00,  5.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  4.19it/s]\n",
            "                   all        164        164      0.998      0.996      0.995      0.749\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/10      1.07G     0.8828      0.561      1.098          4        256: 100% 109/109 [00:16<00:00,  6.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.74it/s]\n",
            "                   all        164        164      0.995      0.995      0.995      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/10      1.07G     0.8528     0.5278      1.074          6        256: 100% 109/109 [00:18<00:00,  5.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.82it/s]\n",
            "                   all        164        164      0.996      0.995      0.995      0.793\n",
            "\n",
            "10 epochs completed in 0.059 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.117 üöÄ Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11127132 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.43it/s]\n",
            "                   all        164        164      0.995      0.995      0.995      0.826\n",
            "               jogging        164         36      0.987          1      0.995      0.838\n",
            "               sitting        164         43          1      0.982      0.995       0.79\n",
            "              standing        164         45      0.994          1      0.995      0.864\n",
            "               walking        164         40          1          1      0.995       0.81\n",
            "Speed: 0.4ms preprocess, 2.2ms inference, 0.0ms loss, 5.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! wandb disabled\n",
        "\n",
        "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=10 imgsz=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce7U7VqGm5iv",
        "outputId": "5b35f47a-e574-411f-b310-db16806e0153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.117 üöÄ Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11127132 parameters, 0 gradients\n",
            "\n",
            "image 1/86 /content/human-activity-detection-2-3/test/images/jogg-1033_jpg.rf.7f56611287872ecafde9deb247a0a143.jpg: 256x256 1 jogging, 10.0ms\n",
            "image 2/86 /content/human-activity-detection-2-3/test/images/jogg-108_jpg.rf.270f01c08c6dc3eac5cba6a0a76ee922.jpg: 256x256 1 jogging, 7.1ms\n",
            "image 3/86 /content/human-activity-detection-2-3/test/images/jogg-1111_jpg.rf.a3e153175be0ffe9d175b4599ac99351.jpg: 256x256 1 jogging, 7.4ms\n",
            "image 4/86 /content/human-activity-detection-2-3/test/images/jogg-1120_jpg.rf.ddef13bbd050097a9240ba5cf3cadec4.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 5/86 /content/human-activity-detection-2-3/test/images/jogg-114_jpg.rf.142ac030857273fece978db86cf969be.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 6/86 /content/human-activity-detection-2-3/test/images/jogg-1303_jpg.rf.6d27af55c67284b7ab50cd75bbea8b32.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 7/86 /content/human-activity-detection-2-3/test/images/jogg-1308_jpg.rf.a9e4367660cdbbe5c24c66c2e0b3b16d.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 8/86 /content/human-activity-detection-2-3/test/images/jogg-1313_jpg.rf.54ca480df0d3934be0119f85211b22a7.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 9/86 /content/human-activity-detection-2-3/test/images/jogg-1323_jpg.rf.28fd50608ff5415d598fabfb76b5db87.jpg: 256x256 1 jogging, 7.1ms\n",
            "image 10/86 /content/human-activity-detection-2-3/test/images/jogg-1326_jpg.rf.99bbc18f47727ed29f18e90b604b66dc.jpg: 256x256 1 jogging, 1 walking, 7.0ms\n",
            "image 11/86 /content/human-activity-detection-2-3/test/images/jogg-1344_jpg.rf.c5a250d4dc30e4469da05073a6966ffb.jpg: 256x256 1 jogging, 8.4ms\n",
            "image 12/86 /content/human-activity-detection-2-3/test/images/jogg-1801_jpg.rf.6496cfa68a0dc89f059025028a6dae82.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 13/86 /content/human-activity-detection-2-3/test/images/jogg-1802_jpg.rf.cea5adc5f64092810e623672f5929ea3.jpg: 256x256 1 jogging, 6.9ms\n",
            "image 14/86 /content/human-activity-detection-2-3/test/images/jogg-1902_jpg.rf.8257e67aa410eaad06c9936c76d82b13.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 15/86 /content/human-activity-detection-2-3/test/images/jogg-421_jpg.rf.648c1c7a342a24b04370070794fd0e65.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 16/86 /content/human-activity-detection-2-3/test/images/jogg-431_jpg.rf.f6321a2788fae7af97a81e550aaa6304.jpg: 256x256 1 jogging, 7.1ms\n",
            "image 17/86 /content/human-activity-detection-2-3/test/images/jogg-434_jpg.rf.52f6c82c1c8e097dae0793d98b14989f.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 18/86 /content/human-activity-detection-2-3/test/images/jogg-506_jpg.rf.91f72e201314e848662a56fac7fe24e0.jpg: 256x256 1 jogging, 6.9ms\n",
            "image 19/86 /content/human-activity-detection-2-3/test/images/jogg-805_jpg.rf.57ad1317db7d577a70e011882a0e3e7b.jpg: 256x256 1 jogging, 7.0ms\n",
            "image 20/86 /content/human-activity-detection-2-3/test/images/jogg-818_jpg.rf.cf1f565846cdb100a84f60a07e6feab1.jpg: 256x256 1 jogging, 6.9ms\n",
            "image 21/86 /content/human-activity-detection-2-3/test/images/jogg-826_jpg.rf.1557a3b2b2befc08b0df9e2cd29656c8.jpg: 256x256 1 jogging, 6.9ms\n",
            "image 22/86 /content/human-activity-detection-2-3/test/images/sit-104_jpg.rf.421bbdb72233e5b08e4f08bfd0e7e1b1.jpg: 256x256 1 sitting, 6.9ms\n",
            "image 23/86 /content/human-activity-detection-2-3/test/images/sit-112_jpg.rf.1f4a2e1be628bbcf72e0b415e96488d6.jpg: 256x256 1 sitting, 7.1ms\n",
            "image 24/86 /content/human-activity-detection-2-3/test/images/sit-119_jpg.rf.3023a860ef9ccbd1ce4fde8b47908ca4.jpg: 256x256 1 sitting, 6.9ms\n",
            "image 25/86 /content/human-activity-detection-2-3/test/images/sit-153_jpg.rf.d1c5591ee342e721007d91c3177230ab.jpg: 256x256 1 sitting, 6.9ms\n",
            "image 26/86 /content/human-activity-detection-2-3/test/images/sit-217_jpg.rf.b90c6bdf9db09eac4efe02b0fc2fe63f.jpg: 256x256 1 sitting, 6.9ms\n",
            "image 27/86 /content/human-activity-detection-2-3/test/images/sit-4133_jpg.rf.4785a34ca753ab6b4af49175c9877386.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 28/86 /content/human-activity-detection-2-3/test/images/sit-4135_jpg.rf.6538ce43d5e2fe42ec13cf36b525c176.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 29/86 /content/human-activity-detection-2-3/test/images/sit-4145_jpg.rf.ce002037cfe495ad4bbfdff56856eb8e.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 30/86 /content/human-activity-detection-2-3/test/images/sit-5041_jpg.rf.01d510b56dd74a352f47cf5385002576.jpg: 256x256 1 sitting, 7.1ms\n",
            "image 31/86 /content/human-activity-detection-2-3/test/images/sit-5057_jpg.rf.834c3db7f4834b038b43098348e440f0.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 32/86 /content/human-activity-detection-2-3/test/images/sit-5065_jpg.rf.12a6152fcc90f829626f2012fe8e269a.jpg: 256x256 1 sitting, 7.3ms\n",
            "image 33/86 /content/human-activity-detection-2-3/test/images/sit-5079_jpg.rf.55b2945256fab2a656d9847748b2a29f.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 34/86 /content/human-activity-detection-2-3/test/images/sit-5088_jpg.rf.38f0f5df5840f6a74ea61b10bbd74252.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 35/86 /content/human-activity-detection-2-3/test/images/sit-5094_jpg.rf.d9e014e102d659fb4281466c13e303fe.jpg: 256x256 1 sitting, 7.5ms\n",
            "image 36/86 /content/human-activity-detection-2-3/test/images/sit-8108_jpg.rf.4602d107d64b3830411c55e5f063b3b9.jpg: 256x256 1 sitting, 6.9ms\n",
            "image 37/86 /content/human-activity-detection-2-3/test/images/sit-8109_jpg.rf.fb6f9ad61e58bd029b284bd666767a8e.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 38/86 /content/human-activity-detection-2-3/test/images/sit-8120_jpg.rf.23cf0b1678e2a587c91d7988b2550868.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 39/86 /content/human-activity-detection-2-3/test/images/sit-8124_jpg.rf.6088d338ea1277d275cbb5bca01f1e34.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 40/86 /content/human-activity-detection-2-3/test/images/sit-8127_jpg.rf.7d5c673435fee72a5348a9ca761ef6aa.jpg: 256x256 1 sitting, 6.9ms\n",
            "image 41/86 /content/human-activity-detection-2-3/test/images/sit-8133_jpg.rf.0a1cfe1be32008a719683491405d6440.jpg: 256x256 1 sitting, 7.0ms\n",
            "image 42/86 /content/human-activity-detection-2-3/test/images/sit-9137_jpg.rf.f7d74b222ed21e4f42b24dd89215ac50.jpg: 256x256 1 sitting, 6.9ms\n",
            "image 43/86 /content/human-activity-detection-2-3/test/images/stand-103_jpg.rf.2e68e1e9e7f96fa87205cdfc0cbf15f3.jpg: 256x256 1 standing, 7.0ms\n",
            "image 44/86 /content/human-activity-detection-2-3/test/images/stand-121_jpg.rf.5a6931002f655eb21a8da16d24f7e4d1.jpg: 256x256 1 standing, 9.6ms\n",
            "image 45/86 /content/human-activity-detection-2-3/test/images/stand-3209_jpg.rf.3435cf37c58aef5a067c769005fb0a77.jpg: 256x256 1 standing, 7.0ms\n",
            "image 46/86 /content/human-activity-detection-2-3/test/images/stand-3211_jpg.rf.67fb01925dec547d0b01475850282ec9.jpg: 256x256 1 standing, 6.9ms\n",
            "image 47/86 /content/human-activity-detection-2-3/test/images/stand-3219_jpg.rf.da7d8b947d9d6861a6ff9d4079ba86f1.jpg: 256x256 1 standing, 7.5ms\n",
            "image 48/86 /content/human-activity-detection-2-3/test/images/stand-3233_jpg.rf.03fd660c6c99bc7e04cdc953f83d36a6.jpg: 256x256 1 standing, 7.0ms\n",
            "image 49/86 /content/human-activity-detection-2-3/test/images/stand-4003_jpg.rf.1f6746774ce027b23f3ae52acfce878f.jpg: 256x256 1 standing, 7.0ms\n",
            "image 50/86 /content/human-activity-detection-2-3/test/images/stand-4012_jpg.rf.b64139f34cc6e8835aa9610835c417aa.jpg: 256x256 1 standing, 7.0ms\n",
            "image 51/86 /content/human-activity-detection-2-3/test/images/stand-4036_jpg.rf.395599383d9ff58687a4dfa86ec42ae0.jpg: 256x256 1 standing, 6.9ms\n",
            "image 52/86 /content/human-activity-detection-2-3/test/images/stand-4052_jpg.rf.eedefaa8e29e4ba12a04482f197196ca.jpg: 256x256 1 standing, 7.0ms\n",
            "image 53/86 /content/human-activity-detection-2-3/test/images/stand-4053_jpg.rf.fd64888ec6065cea932cadbaf5359474.jpg: 256x256 1 standing, 7.0ms\n",
            "image 54/86 /content/human-activity-detection-2-3/test/images/stand-4066_jpg.rf.dbc1ec1f22288bafbc88917463f82747.jpg: 256x256 1 standing, 7.0ms\n",
            "image 55/86 /content/human-activity-detection-2-3/test/images/stand-4073_jpg.rf.ba94cbd943d7ee2ce74295a700145ed2.jpg: 256x256 1 standing, 6.9ms\n",
            "image 56/86 /content/human-activity-detection-2-3/test/images/stand-4075_jpg.rf.ed6bfba20272038fbb94e6b6afc5a223.jpg: 256x256 1 standing, 6.9ms\n",
            "image 57/86 /content/human-activity-detection-2-3/test/images/stand-4092_jpg.rf.52f9e96de26ca0b3687095ce9da46318.jpg: 256x256 1 standing, 6.9ms\n",
            "image 58/86 /content/human-activity-detection-2-3/test/images/stand-4098_jpg.rf.0313c90929a3beef7937a34a93590b45.jpg: 256x256 1 standing, 7.2ms\n",
            "image 59/86 /content/human-activity-detection-2-3/test/images/stand-4108_jpg.rf.7ba7921aeebe870c413fc8dbace0629b.jpg: 256x256 1 standing, 7.8ms\n",
            "image 60/86 /content/human-activity-detection-2-3/test/images/walk-102_jpg.rf.afc73062b88a63de6015320af0fc454c.jpg: 256x256 1 walking, 6.9ms\n",
            "image 61/86 /content/human-activity-detection-2-3/test/images/walk-118_jpg.rf.131336e0397485d7c894d3088acb1b9d.jpg: 256x256 1 walking, 7.0ms\n",
            "image 62/86 /content/human-activity-detection-2-3/test/images/walk-119_jpg.rf.a36824bde998932651b00860715ca25d.jpg: 256x256 1 walking, 7.1ms\n",
            "image 63/86 /content/human-activity-detection-2-3/test/images/walk-124_jpg.rf.3e7d6753dcb2720861160738caefff72.jpg: 256x256 1 walking, 6.9ms\n",
            "image 64/86 /content/human-activity-detection-2-3/test/images/walk-128_jpg.rf.00508684a9be9213370e270932da42fe.jpg: 256x256 1 walking, 7.1ms\n",
            "image 65/86 /content/human-activity-detection-2-3/test/images/walk-1402_jpg.rf.169fc5246fc4751fca7acf1e36b1b3f5.jpg: 256x256 1 walking, 6.9ms\n",
            "image 66/86 /content/human-activity-detection-2-3/test/images/walk-1403_jpg.rf.fbb2e9ad5b37d050cb41e6bdb4de70d5.jpg: 256x256 1 walking, 7.0ms\n",
            "image 67/86 /content/human-activity-detection-2-3/test/images/walk-1501_jpg.rf.619cdf86fb7611bdf46e72cd1bde70e6.jpg: 256x256 1 walking, 7.0ms\n",
            "image 68/86 /content/human-activity-detection-2-3/test/images/walk-1502_jpg.rf.8f0b6833b4a20a41aa880639b76b81f9.jpg: 256x256 1 walking, 6.9ms\n",
            "image 69/86 /content/human-activity-detection-2-3/test/images/walk-1505_jpg.rf.cb873f19834d480d5bdb424728697cf0.jpg: 256x256 1 walking, 6.9ms\n",
            "image 70/86 /content/human-activity-detection-2-3/test/images/walk-1517_jpg.rf.7919e17f4f4f5039da37027255c6ff95.jpg: 256x256 1 walking, 6.9ms\n",
            "image 71/86 /content/human-activity-detection-2-3/test/images/walk-1603_jpg.rf.a6b3f4e1cb0972e0229d39ab3658e9ca.jpg: 256x256 1 walking, 7.0ms\n",
            "image 72/86 /content/human-activity-detection-2-3/test/images/walk-1608_jpg.rf.3fb8277a540467d789cf10840dc0d7c9.jpg: 256x256 1 walking, 7.0ms\n",
            "image 73/86 /content/human-activity-detection-2-3/test/images/walk-1806_jpg.rf.2aa8b8604aa0173612313fb774d8f306.jpg: 256x256 1 walking, 7.5ms\n",
            "image 74/86 /content/human-activity-detection-2-3/test/images/walk-1807_jpg.rf.935aff9416d835cdc3a4f00a2ba1f45d.jpg: 256x256 1 walking, 7.0ms\n",
            "image 75/86 /content/human-activity-detection-2-3/test/images/walk-1900_jpg.rf.4f550c3630d72fff9372d2c8ca441a94.jpg: 256x256 1 walking, 7.0ms\n",
            "image 76/86 /content/human-activity-detection-2-3/test/images/walk-1910_jpg.rf.9f9eaf6969510365654504dd5fc8573b.jpg: 256x256 1 walking, 7.0ms\n",
            "image 77/86 /content/human-activity-detection-2-3/test/images/walk-1912_jpg.rf.3c5464cb7604b5ae23c8abaf37280252.jpg: 256x256 1 walking, 8.2ms\n",
            "image 78/86 /content/human-activity-detection-2-3/test/images/walk-211_jpg.rf.c907ff5655d8fcb87cdfb1c308ff2fdb.jpg: 256x256 1 walking, 7.0ms\n",
            "image 79/86 /content/human-activity-detection-2-3/test/images/walk-216_jpg.rf.3454498dde1db4809e596a551e6f7a11.jpg: 256x256 1 walking, 6.9ms\n",
            "image 80/86 /content/human-activity-detection-2-3/test/images/walk-848_jpg.rf.6aeb9cfa9655dfae510dbf4086774130.jpg: 256x256 1 walking, 7.2ms\n",
            "image 81/86 /content/human-activity-detection-2-3/test/images/walk-853_jpg.rf.27d795a88e55fa3c3fb0aaf29747216b.jpg: 256x256 1 walking, 7.0ms\n",
            "image 82/86 /content/human-activity-detection-2-3/test/images/walk-900_jpg.rf.f3ce7bbe38ea7de4d2bc5fb586daaa50.jpg: 256x256 1 walking, 8.9ms\n",
            "image 83/86 /content/human-activity-detection-2-3/test/images/walk-915_jpg.rf.2d771b001653297778796d29492ca95c.jpg: 256x256 1 walking, 7.1ms\n",
            "image 84/86 /content/human-activity-detection-2-3/test/images/walk-926_jpg.rf.d07ef4c3fda67e5daacc3e4fe778ce09.jpg: 256x256 1 walking, 7.0ms\n",
            "image 85/86 /content/human-activity-detection-2-3/test/images/walk-931_jpg.rf.769cc8fbb21855866a52f327e079a002.jpg: 256x256 1 walking, 7.0ms\n",
            "image 86/86 /content/human-activity-detection-2-3/test/images/walk-943_jpg.rf.86d54efec6ba29b41067b0ce1ff3a968.jpg: 256x256 1 walking, 7.0ms\n",
            "Speed: 0.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!yolo task=detect mode=predict model=/content/runs/detect/train/weights/best.pt conf=0.20 source={dataset.location}/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
